{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87f89a28",
      "metadata": {},
      "source": [
        "<img src=\"assets/projet7_logo.svg\" alt=\"Projet 7\" style=\"float: right; width: 120px; margin: 0 0 0 12px;\" />\n",
        "\n",
        "# Projet 7 \n",
        "**Auteur : El-yassa Zebakh**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec835701",
      "metadata": {},
      "source": [
        "# Home Credit Scoring — Notebook de modélisation\n",
        "\n",
        "Notebook rédigé pour documenter le pipeline de scoring construit sur le dataset Home Credit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82d0ec00",
      "metadata": {},
      "source": [
        "**Ce que fait ce notebook**\n",
        "- Charge le dataset `application_train.csv` et dresse un diagnostic rapide (types, valeurs manquantes, distribution de la cible TARGET).\n",
        "- Met en place un pipeline de prétraitement scikit-learn (numérique + catégoriel) en évitant toute fuite de données.\n",
        "- Entraîne plusieurs modèles (régression logistique, gradient boosting, XGBoost/LightGBM) via RandomizedSearchCV.\n",
        "- Optimise un seuil métier basé sur un coût FN/FP asymétrique, sélectionne un modèle champion et logue les résultats dans MLflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba0d791",
      "metadata": {},
      "source": [
        "### Sommaire\n",
        "- [1. Import des dépendances et chargement initial des données](#1-import-des-dependances-et-chargement-initial-des-donnees)\n",
        "- [2. Typologie des variables et valeurs manquantes](#2-typologie-des-variables-et-valeurs-manquantes)\n",
        "- [3. Cible métier (TARGET)](#3-cible-metier-target)\n",
        "- [4. Prétraitement des données](#4-pretraitement-des-donnees-pipelines-scikit-learn)\n",
        "- [4.1 Règles générales et split train/validation/test](#41-regles-generales-et-split-trainvalidationtest)\n",
        "- [4.2 Séparer numériques / catégorielles](#42-separer-numeriques-categorielles)\n",
        "- [4.3 Pipeline numérique](#43-pipeline-numerique)\n",
        "- [4.4 Pipeline catégoriel](#44-pipeline-categoriel)\n",
        "- [4.5 ColumnTransformer et pipeline global](#45-columntransformer-et-pipeline-global)\n",
        "- [5. Modèles candidats et recherche d'hyperparamètres](#5-modeles-candidats-et-recherche-dhyperparametres)\n",
        "- [6. Évaluation validation : AUC et coût métier](#6-evaluation-validation-auc-et-cout-metier)\n",
        "- [7. Sélection du modèle champion et interprétation métier](#7-selection-du-modele-champion-et-interpretation-metier)\n",
        "- [8. Suivi expérimental avec MLflow](#8-suivi-experimental-avec-mlflow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be50719",
      "metadata": {},
      "source": [
        "## <a id=\"1-import-des-dependances-et-chargement-initial-des-donnees\"></a>1. Import des dépendances et chargement initial des données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95008500",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import missingno as msno\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# On suppose que le notebook est dans HOME_CREDIT_PROJECT/notebooks\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
        "MLFLOW_DIR = PROJECT_ROOT / \"mlruns\"\n",
        "\n",
        "# On ajoute la racine du projet au sys.path \n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
        "print(\"src in sys.path ?\", any(\"home_credit_project\" in p and \"src\" in p for p in sys.path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94537ebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/raw/application_train.csv')\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe()\n",
        "df.isnull().sum()\n",
        "msno.bar(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72a9239",
      "metadata": {},
      "source": [
        "## <a id=\"2-typologie-des-variables-et-valeurs-manquantes\"></a>2. Typologie des variables et valeurs manquantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec94716",
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_df = df.select_dtypes(include=['object', 'category', 'bool'])\n",
        "numerical_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "print(f\"Number of categorical features: {categorical_df.shape[1]}\")\n",
        "print(f\"Number of numerical features: {numerical_df.shape[1]}\")\n",
        "\n",
        "missing_summary = (\n",
        "    df.isna()\n",
        "      .mean()\n",
        "      .mul(100)\n",
        "      .rename('missing_pct')\n",
        "      .round(2)\n",
        "      .to_frame()\n",
        ")\n",
        "\n",
        "categorical_cardinality = (\n",
        "    categorical_df.apply(lambda s: s.nunique(dropna=True))\n",
        "      .rename('unique_categories')\n",
        "      .sort_values(ascending=False)\n",
        "      .to_frame()\n",
        ")\n",
        "\n",
        "with pd.option_context('display.max_rows', 122):\n",
        "    display(missing_summary[missing_summary['missing_pct'] > 0].sort_values(by='missing_pct', ascending=False))\n",
        "display(categorical_cardinality)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec3e500",
      "metadata": {},
      "source": [
        "## <a id=\"3-cible-metier-target\"></a>3. Cible métier (TARGET)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5570786c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "TARGET_COL = 'TARGET'\n",
        "\n",
        "value_counts = df[TARGET_COL].value_counts(dropna=False)\n",
        "percentages = df[TARGET_COL].value_counts(normalize=True, dropna=False).mul(100).round(2)\n",
        "\n",
        "target_summary = (\n",
        "    pd.DataFrame({'count': value_counts, 'percent': percentages})\n",
        "      .sort_index()\n",
        ")\n",
        "\n",
        "print(f\"Value counts for target '{TARGET_COL}':\")\n",
        "display(target_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaac8a6a",
      "metadata": {},
      "source": [
        "## <a id=\"4-pretraitement-des-donnees-pipelines-scikit-learn\"></a>4. Prétraitement des données (pipelines scikit-learn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8518e59e",
      "metadata": {},
      "source": [
        "Prétraitement des variables (pipeline scikit-learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d72d88f",
      "metadata": {},
      "source": [
        "### <a id=\"41-regles-generales-et-split-trainvalidationtest\"></a>4.1 Règles générales et split train/validation/test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d819bf47",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Afin de garantir une démarche reproductible et d’éviter toute fuite de données (data leakage), \n",
        "l’ensemble des étapes de prétraitement a été encapsulé dans un pipeline scikit-learn. Ce pipeline est systématiquement appris sur le jeu d’entraînement uniquement, \n",
        "puis appliqué aux jeux de validation, de test et, à terme, aux nouvelles demandes de crédit.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d13981",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# 1) Train/Test split avec stratification\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "# 2) Train/Validation split basé sur la portion train_val (aussi stratifié)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.2,  \n",
        "    random_state=42,\n",
        "    stratify=y_train_val,\n",
        ")\n",
        "\n",
        "print('Shapes:')\n",
        "print(' X_train:', X_train.shape, ' y_train:', y_train.shape)\n",
        "print(' X_valid:', X_valid.shape, ' y_valid:', y_valid.shape)\n",
        "print(' X_test :', X_test.shape,  ' y_test :', y_test.shape)\n",
        "\n",
        "print('\\nClass balance (%):')\n",
        "for name, yy in [('train', y_train), ('valid', y_valid), ('test', y_test)]:\n",
        "    pct = (yy.value_counts(normalize=True) * 100).round(2)\n",
        "    print(f' {name}:')\n",
        "    print(pct.sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd32ca6",
      "metadata": {},
      "source": [
        "### <a id=\"42-separer-numeriques-categorielles\"></a>4.2 Séparer numériques / catégorielles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee9bb00f",
      "metadata": {},
      "source": [
        "Séparation numériques / catégorielles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62fc2138",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Les variables explicatives ont été séparées en deux groupes :\n",
        "\t•\tvariables numériques (montants, durées, scores, ratios…),\n",
        "\t•\tvariables catégorielles (type de contrat, type de logement, situation familiale, etc.).\n",
        "\n",
        "Les colonnes techniques, telles que l’identifiant de dossier SK_ID_CURR, ainsi que les variables présentant plus de 60 % de valeurs manquantes,\n",
        " ont été supprimées sur la base du jeu d’entraînement, puis ce même choix de variables a été appliqué aux jeux de validation et de test.\n",
        " \n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e104c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "MISSING_THRESHOLD = 0.60\n",
        "\n",
        "missing_ratio_train = X_train.isna().mean()\n",
        "cols_to_drop = missing_ratio_train[missing_ratio_train > MISSING_THRESHOLD].index.tolist()\n",
        "\n",
        "\n",
        "if 'SK_ID_CURR' in X_train.columns and 'SK_ID_CURR' not in cols_to_drop:\n",
        "    cols_to_drop.append('SK_ID_CURR')\n",
        "\n",
        "print(f\"Columns to drop (> {int(MISSING_THRESHOLD*100)}% missing + SK_ID_CURR): {len(cols_to_drop)}\")\n",
        "\n",
        "X_train = X_train.drop(columns=cols_to_drop)\n",
        "X_valid = X_valid.drop(columns=cols_to_drop)\n",
        "X_test  = X_test.drop(columns=cols_to_drop)\n",
        "\n",
        "feature_df = X_train  \n",
        "\n",
        "numeric_features = feature_df.select_dtypes(include=['number']).columns.tolist()\n",
        "categorical_features = feature_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features: {len(numeric_features)} | Categorical features: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b297a97",
      "metadata": {},
      "source": [
        "### <a id=\"43-pipeline-numerique\"></a>4.3 Pipeline numérique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383c7401",
      "metadata": {},
      "source": [
        "Prétraitement des variables numériques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63361fa9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "\t1.\tImputation des valeurs manquantes par la médiane :\n",
        "La médiane est calculée sur le jeu d’entraînement uniquement, ce qui la rend robuste aux valeurs extrêmes fréquemment observées dans les données financières \n",
        "(revenus très élevés, montants de crédit atypiques).\n",
        "\n",
        "\t2.\tStandardisation (centrage-réduction) :\n",
        "Chaque variable est centrée et réduite (moyenne 0, écart-type 1) à partir des statistiques du jeu d’entraînement. \n",
        "Cette étape est particulièrement importante pour les modèles linéaires tels que la régression logistique, afin d’éviter que certaines variables dominent numériquement \n",
        "les autres et de faciliter la convergence de l’algorithme d’optimisation.”\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaf672d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ca1ae6",
      "metadata": {},
      "source": [
        "### <a id=\"44-pipeline-categoriel\"></a>4.4 Pipeline catégoriel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229c1d8d",
      "metadata": {},
      "source": [
        "Prétraitement des variables catégorielles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9214f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "\t1.\tImputation des valeurs manquantes :\n",
        "Les valeurs manquantes sont remplacées par une modalité dédiée ‘MISSING’, calculée à partir du jeu d’entraînement.\n",
        "\n",
        "\t2.\tEncodage One-Hot :\n",
        "Les variables catégorielles sont ensuite transformées en indicatrices binaires via un encodage One-Hot. Afin d’assurer la robustesse du modèle en production, \n",
        "les catégories jamais observées lors de l’entraînement sont explicitement gérées (elles sont ignorées lors de l’inférence, ce qui évite les erreurs).”\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020cfd65",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f43af2",
      "metadata": {},
      "source": [
        "### <a id=\"45-columntransformer-et-pipeline-global\"></a>4.5 ColumnTransformer et pipeline global\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2388568",
      "metadata": {},
      "source": [
        "ColumnTransformer et pipeline global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09529bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Les deux pipelines (numérique et catégoriel) sont combinés à l’aide d’un ColumnTransformer, qui applique en parallèle le traitement adapté à chaque groupe de variables, \n",
        "puis concatène le tout pour obtenir une matrice de caractéristiques prête à être exploitée par les modèles.\n",
        "\n",
        "Ce préprocesseur global est ensuite intégré comme première étape dans l’ensemble des pipelines de modèles (régression logistique, Gradient Boosting, XGBoost/LightGBM). \n",
        "Ainsi, lors de la cross-validation et de l’optimisation des hyperparamètres (RandomizedSearchCV), ce sont toujours les données brutes qui sont fournies au pipeline, \n",
        "et le prétraitement est réappris à chaque fold à partir du sous-jeu d’entraînement du fold. Cela garantit une évaluation honnête des performances et une absence de fuite de données.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda32349",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numeric_transformer, numeric_features),\n",
        "        ('categorical', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "preprocessor.set_output(transform='pandas')\n",
        "\n",
        "X_train_prepared = preprocessor.fit_transform(X_train)\n",
        "X_valid_prepared = preprocessor.transform(X_valid)\n",
        "X_test_prepared = preprocessor.transform(X_test)\n",
        "\n",
        "X_train_prepared.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd5af8a",
      "metadata": {},
      "source": [
        "## <a id=\"5-modeles-candidats-et-recherche-dhyperparametres\"></a>5. Modèles candidats et recherche d'hyperparamètres\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398f0390",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "preprocessor = globals().get('preprocessor')\n",
        "\n",
        "\n",
        "y_train_values = y_train.to_numpy()\n",
        "classes = np.unique(y_train_values)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_values)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "# scale_pos_weight pour XGBoost \n",
        "if len(classes) == 2:\n",
        "    counts = y_train.value_counts()\n",
        "    # positive class assumed to be the larger label value\n",
        "    majority_class = counts.idxmax()\n",
        "    minority_class = counts.idxmin()\n",
        "    scale_pos_weight = counts[majority_class] / counts[minority_class]\n",
        "else:\n",
        "    scale_pos_weight = 1.0\n",
        "\n",
        "pipeline_lr = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(\n",
        "            class_weight=class_weight_dict,\n",
        "            max_iter=3000,\n",
        "            solver='lbfgs'\n",
        "        )),\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline_gb = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', HistGradientBoostingClassifier(\n",
        "            class_weight=class_weight_dict,\n",
        "            random_state=42\n",
        "        )),\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline_xgb = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            random_state=42,\n",
        "            tree_method='hist',\n",
        "            njobs=1\n",
        "        )),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\" J’ai encapsulé le prétraitement et le modèle dans un pipeline unique, pour être sûr que la cross-validation et le déploiement utilisent exactement les mêmes transformations.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72ba8841",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import loguniform, randint, uniform\n",
        "\n",
        "pipeline_lr = globals().get('pipeline_lr')\n",
        "pipeline_gb = globals().get('pipeline_gb')\n",
        "pipeline_xgb = globals().get('pipeline_xgb')\n",
        "\n",
        "\n",
        "\n",
        "param_distributions_lr = {\n",
        "    'classifier__C': loguniform(1e-2, 1e2),\n",
        "    'classifier__tol': loguniform(1e-6, 1e-3),\n",
        "}\n",
        "\n",
        "param_distributions_gb = {\n",
        "    'classifier__learning_rate': loguniform(1e-3, 1e0),\n",
        "    'classifier__max_depth': randint(2, 12),\n",
        "    'classifier__min_samples_leaf': randint(20, 200),\n",
        "    'classifier__l2_regularization': loguniform(1e-6, 1e0),\n",
        "    'classifier__max_leaf_nodes': randint(16, 128),\n",
        "}\n",
        "\n",
        "param_distributions_xgb = {\n",
        "    'classifier__n_estimators': randint(200, 600),\n",
        "    'classifier__max_depth': randint(3, 10),\n",
        "    'classifier__min_child_weight': randint(1, 10),\n",
        "    'classifier__gamma': loguniform(1e-3, 1e1),\n",
        "    'classifier__subsample': uniform(0.6, 0.4),\n",
        "    'classifier__colsample_bytree': uniform(0.6, 0.4),\n",
        "    'classifier__reg_lambda': loguniform(1e-3, 1e2),\n",
        "    'classifier__reg_alpha': loguniform(1e-3, 1e1),\n",
        "    'classifier__learning_rate': loguniform(1e-3, 5e-1),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Pour chaque algorithme, j’ai défini un espace d’hyperparamètres réaliste (nombre d’arbres, profondeur, taux d’apprentissage, etc.) que RandomizedSearchCV va explorer, \n",
        "avec comme métrique principale l’AUC.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbb4886",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "search_spaces = {\n",
        "    'pipeline_lr': RandomizedSearchCV(\n",
        "        estimator=pipeline_lr,\n",
        "        param_distributions=param_distributions_lr,\n",
        "        n_iter=30,\n",
        "        scoring='roc_auc',\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1,\n",
        "    ),\n",
        "    'pipeline_gb': RandomizedSearchCV(\n",
        "        estimator=pipeline_gb,\n",
        "        param_distributions=param_distributions_gb,\n",
        "        n_iter=40,\n",
        "        scoring='roc_auc',\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1,\n",
        "    ),\n",
        "    'pipeline_xgb': RandomizedSearchCV(\n",
        "        estimator=pipeline_xgb,\n",
        "        param_distributions=param_distributions_xgb,\n",
        "        n_iter=50,\n",
        "        scoring='roc_auc',\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1,\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Dictionnaire pour stocker les meilleurs résultats et modèles\n",
        "best_results = dict()\n",
        "\n",
        "# On boucle sur les modèles et on fit/test chaque RandomizedSearchCV\n",
        "for name, search in search_spaces.items():\n",
        "    print(f\"Fitting {name} ...\")\n",
        "    search.fit(X_train, y_train)  \n",
        "    best_results[name] = {\n",
        "        'best_estimator': search.best_estimator_,\n",
        "        'best_score': search.best_score_,\n",
        "        'best_params': search.best_params_,\n",
        "    }\n",
        "    print(f\"{name} -> Best AUC: {search.best_score_:.4f}\")\n",
        "\n",
        "# Accès facile au meilleur modèle/score/params par nom de pipeline :\n",
        "# best_results['pipeline_lr']['best_estimator'], etc.\n",
        "\n",
        "best_results\n",
        "\n",
        "\"\"\" Pour chaque pipeline (prétraitement + modèle), j’ai lancé un RandomizedSearchCV avec une cross-validation stratifiée sur le jeu d’entraînement et un scoring AUC, \n",
        "ce qui me permet de comparer les modèles à partir d’une métrique robuste sur données déséquilibrées. \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9deecc",
      "metadata": {},
      "source": [
        "## <a id=\"6-evaluation-validation-auc-et-cout-metier\"></a>6. Évaluation validation : AUC et coût métier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0413b789",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix,  precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Get predictions for each best estimator on validation set\n",
        "y_proba_dict = {}\n",
        "for name, results in best_results.items():\n",
        "    estimator = results['best_estimator']\n",
        "    y_proba = estimator.predict_proba(X_valid)[:, 1]  \n",
        "    y_proba_dict[name] = y_proba\n",
        "    print(f\"{name}: Computed probabilities for {len(y_proba)} validation samples\")\n",
        "\n",
        "def evaluate_with_cost(y_true, y_proba, threshold=0.5, c_fn=10, c_fp=1):\n",
        "    \n",
        "    # Predict based on threshold\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # Calculate costs\n",
        "    total_cost = (fn * c_fn) + (fp * c_fp)\n",
        "    n_samples = len(y_true)\n",
        "    avg_cost_per_customer = total_cost / n_samples\n",
        "    \n",
        "    # Calculate classification metrics\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_true, y_proba)\n",
        "    \n",
        "    results = {\n",
        "        'confusion_matrix': cm,\n",
        "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
        "        'total_cost': total_cost,\n",
        "        'avg_cost_per_customer': avg_cost_per_customer,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'threshold': threshold,\n",
        "        'c_fn': c_fn,\n",
        "        'c_fp': c_fp,\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluation de chaque modèle avec le seuil par défaut = 0,5\n",
        "print(\"=\" * 60)\n",
        "print(\"Evaluation with threshold=0.5, c_fn=10, c_fp=1\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "evaluation_results = {}\n",
        "for name, y_proba in y_proba_dict.items():\n",
        "    results = evaluate_with_cost(y_valid, y_proba, threshold=0.5, c_fn=10, c_fp=1)\n",
        "    evaluation_results[name] = results\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Confusion Matrix:\\n{results['confusion_matrix']}\")\n",
        "    print(f\"  Total Cost: {results['total_cost']:.2f}\")\n",
        "    print(f\"  Avg Cost per Customer: {results['avg_cost_per_customer']:.4f}\")\n",
        "    print(f\"  Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  Recall: {results['recall']:.4f}\")\n",
        "    print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
        "    print(f\"  ROC-AUC: {results['roc_auc']:.4f}\")\n",
        "\n",
        "evaluation_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797f7e10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Maintenant qu'on a évalué chaque modèle avec le seuil par défaut, on cherche le seuil optimal pour minimiser le coût métier\n",
        "\n",
        "# Créer une liste de seuils de 0.01 à 0.99 avec un pas de 0.01\n",
        "thresholds = [round(t, 2) for t in np.arange(0.01, 1.00, 0.01)]\n",
        "\n",
        "# On stock le seuil optimal et les métriques associées pour chaque modèle\n",
        "optimal_results = {}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Finding optimal threshold for each model (minimizing cost)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for name, y_proba in y_proba_dict.items():\n",
        "    print(f\"\\n{name}: Testing {len(thresholds)} thresholds...\")\n",
        "    \n",
        "    # Test all thresholds and find the one with minimum cost\n",
        "    best_cost = float('inf')\n",
        "    best_threshold = None\n",
        "    best_metrics = None\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        results = evaluate_with_cost(y_valid, y_proba, threshold=threshold, c_fn=10, c_fp=1)\n",
        "        cost = results['total_cost']\n",
        "        \n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_threshold = threshold\n",
        "            best_metrics = results\n",
        "    \n",
        "    #  AUC sur validation set\n",
        "    auc_valid = roc_auc_score(y_valid, y_proba)\n",
        "    \n",
        "    # Stockage des métriques\n",
        "    optimal_results[name] = {\n",
        "        'best_threshold': best_threshold,\n",
        "        'cost': best_metrics['total_cost'],\n",
        "        'avg_cost_per_customer': best_metrics['avg_cost_per_customer'],\n",
        "        'recall_1': best_metrics['recall'],  # Recall for class 1\n",
        "        'f1_1': best_metrics['f1_score'],   # F1 for class 1\n",
        "        'precision_1': best_metrics['precision'],\n",
        "        'auc_valid': auc_valid,\n",
        "        'confusion_matrix': best_metrics['confusion_matrix'],\n",
        "        'tp': best_metrics['tp'],\n",
        "        'fp': best_metrics['fp'],\n",
        "        'tn': best_metrics['tn'],\n",
        "        'fn': best_metrics['fn'],\n",
        "    }\n",
        "    \n",
        "    print(f\"  Best threshold: {best_threshold:.2f}\")\n",
        "    print(f\"  Minimal cost: {best_cost:.2f}\")\n",
        "    print(f\"  Avg cost per customer: {best_metrics['avg_cost_per_customer']:.4f}\")\n",
        "    print(f\"  Recall (class 1): {best_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1 (class 1): {best_metrics['f1_score']:.4f}\")\n",
        "    print(f\"  AUC (validation): {auc_valid:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Summary of optimal results:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# dataframe pour une meilleure visu\n",
        "summary_data = []\n",
        "for name, results in optimal_results.items():\n",
        "    summary_data.append({\n",
        "        'model': name,\n",
        "        'best_threshold': results['best_threshold'],\n",
        "        'cost': results['cost'],\n",
        "        'recall_1': results['recall_1'],\n",
        "        'f1_1': results['f1_1'],\n",
        "        'auc_valid': results['auc_valid'],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "display(summary_df)\n",
        "\n",
        "optimal_results\n",
        "\"\"\" Je compare les modèles selon deux axes : une métrique technique (l’AUC sur le jeu de validation) et une métrique métier (coût FN/FP, avec le seuil optimisé). \n",
        "Le choix final du modèle tient compte de ces deux dimensions.\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" Une fois les hyperparamètres optimisés, je n’utilise pas le seuil par défaut de 0.5. J’optimise un seuil spécifique en minimisant un coût métier qui pondère \n",
        "plus fortement les faux négatifs (mauvais clients accordés) que les faux positifs (bons clients refusés). Cela permet de rendre le modèle cohérent avec les enjeux économiques : \n",
        "un crédit accordé à un mauvais payeur coûte plus cher que la perte de marge associée à un refus injustifié. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa6d5ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\" À partir des probabilités de défaut prédites sur le jeu de validation, j’ai optimisé le seuil de décision pour chaque modèle de façon à minimiser un coût métier :\n",
        "- un faux négatif (mauvais client accepté) coûte 10 unités,\n",
        "- un faux positif (bon client refusé) coûte 1 unité.\n",
        "\n",
        "Pour chaque modèle, j’ai balayé les seuils de 0.01 à 0.99, calculé la matrice de confusion et le coût total associé, puis retenu le seuil qui minimise ce coût.\n",
        "J’obtiens ainsi, pour chaque modèle :\n",
        "- un seuil optimisé,\n",
        "- le coût métier minimal atteint,\n",
        "- des métriques classiques (recall, F1 sur la classe 1, précision)\n",
        "- et l’AUC sur le jeu de validation.\n",
        "\n",
        "Ces résultats sont synthétisés dans un tableau comparatif, qui me permet de choisir le modèle final en tenant compte à la fois :\n",
        "- de la performance globale (AUC),\n",
        "- et de l’impact économique des erreurs (coût FN/FP).\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa67c784",
      "metadata": {},
      "source": [
        "## <a id=\"7-selection-du-modele-champion-et-interpretation-metier\"></a>7. Sélection du modèle champion et interprétation métier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e64640",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Étape 1 : choisir le meilleur modèle\n",
        "# Idée : on veut surtout MINIMISER le coût métier,\n",
        "# mais on veut aussi un modèle \"correct\" en AUC et en rappel (recall).\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Sélection du meilleur modèle (coût, AUC et rappel)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# On va calculer un \"score global\" pour chaque modèle.\n",
        "# Plus ce score est petit plus le cout métier est faible.\n",
        "best_model_name = None\n",
        "best_score = float('inf')\n",
        "\n",
        "for name, results in optimal_results.items():\n",
        "    # On normalise le coût pour qu'il soit sur une échelle comparable\n",
        "    # Ici, on divise par 10000 (c'est une approximation) pour éviter d'avoir\n",
        "    # un coût très grand qui écrase tout le reste.\n",
        "\n",
        "    normalized_cost = results['cost'] / 10000.0\n",
        "\n",
        "    # On calcule un score global (composite_score)\n",
        "    # - On veut : coût petit  -> donc on ajoute le coût (avec un poids)\n",
        "    # - On veut : AUC grande  -> donc on SOUSTRAIT l'AUC (car on minimise le score)\n",
        "    # - On veut : recall grand -> donc on SOUSTRAIT le recall\n",
        "    #\n",
        "    # Les poids 0.5, 0.3, 0.2 servent juste à donner plus d'importance au coût.\n",
        "\n",
        "    composite_score = 0.5 * normalized_cost - 0.3 * results['auc_valid'] - 0.2 * results['recall_1']\n",
        "\n",
        "    \"\"\" J’ai exploré les modèles obtenus et retenu celui qui minimise le coût métier tout en gardant une AUC et un rappel satisfaisants. \"\"\"\n",
        "    \n",
        "    print(f\"{name}: cost={results['cost']:.2f}, auc={results['auc_valid']:.4f}, recall={results['recall_1']:.4f}, composite={composite_score:.4f}\")\n",
        "    \n",
        "    if composite_score < best_score:\n",
        "        best_score = composite_score\n",
        "        best_model_name = name\n",
        "\n",
        "print(f\"\\nModèle sélectionné: {best_model_name}\")\n",
        "print(f\"  Seuil (threshold): {optimal_results[best_model_name]['best_threshold']:.2f}\")\n",
        "print(f\"  Coût (validation): {optimal_results[best_model_name]['cost']:.2f}\")\n",
        "print(f\"  AUC (validation): {optimal_results[best_model_name]['auc_valid']:.4f}\")\n",
        "print(f\"  Recall classe 1 (validation): {optimal_results[best_model_name]['recall_1']:.4f}\")\n",
        "\n",
        "# récupérer le meilleur seuil + le meilleur modèle entraîné\n",
        "best_threshold = optimal_results[best_model_name]['best_threshold']\n",
        "best_estimator_from_search = best_results[best_model_name]['best_estimator']\n",
        "best_params = best_results[best_model_name]['best_params']\n",
        "\n",
        "print(f\"\\nMeilleurs hyperparamètres: {best_params}\")\n",
        "\n",
        "#  regrouper train + validation pour ré-entraîner un modèle final\n",
        "# (on utilise plus de données pour apprendre, donc c'est souvent mieux)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"On regroupe TRAIN + VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "X_train_full = pd.concat([X_train, X_valid], axis=0).reset_index(drop=True)\n",
        "y_train_full = pd.concat([y_train, y_valid], axis=0).reset_index(drop=True)\n",
        "\n",
        "print(f\"X_train_full shape: {X_train_full.shape}\")\n",
        "print(f\"y_train_full shape: {y_train_full.shape}\")\n",
        "\n",
        "# créer et entraîner le pipeline final\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Création du pipeline final pour  {best_model_name} \")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Ici, best_estimator_from_search est déjà un pipeline complet\n",
        "# (prétraitement + modèle) avec les bons hyperparamètres\n",
        "final_pipeline = best_estimator_from_search\n",
        "\n",
        "\n",
        "print(\"Entraînement du modèle final sur train_full...\")\n",
        "final_pipeline.fit(X_train_full, y_train_full)\n",
        "print(\"Entraînement terminé!\")\n",
        "\n",
        "# prédire sur le jeu de TEST\n",
        "\n",
        "# On calcule d'abord des probabilités, puis on applique le seuil.\n",
        "\n",
        "print(\"\\nPrédiction des probabilités sur le jeu de TEST...\")\n",
        "y_proba_test = final_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Décision finale avec le meilleur seuil\n",
        "y_pred_test = (y_proba_test >= best_threshold).astype(int)\n",
        "\n",
        "# évaluation finale sur TEST\n",
        "# On calcule le coût + quelques métriques classiques.\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Évaluation finale sur le jeu de TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_results = evaluate_with_cost(y_test, y_proba_test, threshold=best_threshold, c_fn=10, c_fp=1)\n",
        "\n",
        "\n",
        "auc_test = roc_auc_score(y_test, y_proba_test)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "final_test_results = {\n",
        "    'model_name': best_model_name,\n",
        "    'best_threshold': best_threshold,\n",
        "    'auc_test': auc_test,\n",
        "    'cost_test': test_results['total_cost'],\n",
        "    'avg_cost_per_customer_test': test_results['avg_cost_per_customer'],\n",
        "    'recall_1_test': test_results['recall'],\n",
        "    'f1_1_test': test_results['f1_score'],\n",
        "    'precision_1_test': test_results['precision'],\n",
        "    'accuracy_test': accuracy_test,\n",
        "    'confusion_matrix_test': test_results['confusion_matrix'],\n",
        "    'tp': test_results['tp'],\n",
        "    'fp': test_results['fp'],\n",
        "    'tn': test_results['tn'],\n",
        "    'fn': test_results['fn'],\n",
        "}\n",
        "\n",
        "print(\"\\nTest Set Results:\")\n",
        "print(f\"  AUC: {auc_test:.4f}\")\n",
        "print(f\"  Cost: {test_results['total_cost']:.2f}\")\n",
        "print(f\"  Avg Cost per Customer: {test_results['avg_cost_per_customer']:.4f}\")\n",
        "print(f\"  Recall (class 1): {test_results['recall']:.4f}\")\n",
        "print(f\"  F1 (class 1): {test_results['f1_score']:.4f}\")\n",
        "print(f\"  Precision (class 1): {test_results['precision']:.4f}\")\n",
        "print(f\"  Accuracy: {accuracy_test:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_results['confusion_matrix'])\n",
        "\n",
        "final_test_results\n",
        "\n",
        "\"\"\"Une fois le modèle champion et son seuil métier choisis sur validation, je le réentraîne sur l’ensemble train+validation avec les meilleurs hyperparamètres, \n",
        "puis je l’évalue une seule fois sur le jeu de test tenu à part. Cela me donne une estimation honnête de la performance finale, à la fois en termes d’AUC et de coût métier.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9259b4a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "À partir des résultats sur le jeu de validation, j’ai choisi un ‘modèle champion’ en privilégiant d’abord le coût métier (FN beaucoup plus pénalisants que FP), \n",
        "puis en tenant compte de l’AUC et du rappel de la classe 1.\n",
        "\n",
        "Une fois ce modèle et son seuil métier optimisé sélectionnés, je l’ai réentraîné sur l’ensemble du jeu train + validation avec les hyperparamètres trouvés par RandomizedSearchCV.\n",
        "\n",
        "Enfin, j’ai évalué ce modèle final sur le jeu de test, tenu complètement à part, en calculant :\n",
        "\t•\tl’AUC (performance globale de discrimination),\n",
        "\t•\tle coût métier total et moyen par client,\n",
        "\t•\tle rappel, la F1 et la précision sur la classe 1 (clients défaillants),\n",
        "\t•\tl’accuracy et la matrice de confusion.\n",
        "\n",
        "Ce sont ces résultats sur le test qui servent d’estimation finale et honnête de la performance du système de scoring en conditions réelles.\n",
        "\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec72516",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Avec le modèle XGBoost et un seuil métier optimisé à 0,54 :\n",
        "\t•\tl’AUC est de 0,76 sur le jeu de test, ce qui indique une bonne capacité de discrimination,\n",
        "\t•\tle taux de défaut parmi les crédits acceptés est ramené d’environ 8 % à 4,2 %, soit une division par deux du risque,\n",
        "\t•\tle groupe des crédits refusés est fortement enrichi en mauvais payeurs (taux de défaut ≈ 18,5 %),\n",
        "\t•\tle coût métier (10×FN + 1×FP) est réduit d’environ 35 % par rapport à une politique naïve qui accorde le crédit à tout le monde.\n",
        "\n",
        "La précision sur la classe 1 reste faible (≈18,5 %), ce qui est attendu dans un contexte où les défauts sont rares (8 %) et où l’on privilégie la réduction des faux négatifs\n",
        " (mauvais clients acceptés) au prix d’un certain nombre de faux positifs (bons clients refusés).\n",
        "\n",
        "Au regard de la problématique métier – limiter les pertes liées aux défauts de paiement tout en contrôlant l’impact sur les bons clients – \n",
        "le modèle répond à l’objectif : il permet de réduire significativement le risque de défaut sur le portefeuille de crédits accordés, tout en minimisant un coût métier explicitement défini.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cea33b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        " Relire les chiffres clés en langage métier\n",
        "\n",
        "Sur le jeu de test (61 503 clients) avec modèle XGBoost + seuil métier 0,54 :\n",
        "\t•\tMatrice de confusion :\n",
        "\n",
        "\\pmatrice\n",
        "{TN}=42898 & {FP}=13640 \n",
        "{FN}=1868 & {TP}=3097\n",
        "\n",
        "\n",
        "Traduction :\n",
        "\t•\tTN = 42 898 : bons clients correctement acceptés ✅\n",
        "\t•\tFP = 13 640 : bons clients refusés ❌ (manque à gagner)\n",
        "\t•\tFN = 1 868 : mauvais payeurs acceptés ❌ (perte en capital)\n",
        "\t•\tTP = 3 097 : mauvais payeurs refusés ✅\n",
        "\n",
        "Rappels :\n",
        "\t•\tPositifs (1) = mauvais payeurs = 4 965 (8,1 % de la pop)\n",
        "\t•\tNégatifs (0) = bons payeurs = 56 538 (91,9 %)\n",
        "\n",
        "Metrics principales :\n",
        "\t•\tAUC_test = 0,7613\n",
        "\t•\tRecall (classe 1, mauvais payeurs) = 0,6238\n",
        "→ refus : ~62 % des mauvais payeurs.\n",
        "\t•\tPrecision (classe 1) = 0,1850\n",
        "→ parmi les clients refusés, ~18,5 % sont vraiment mauvais.\n",
        "(Donc 81,5 % des refus sont des “bons” qu’on sacrifie)\n",
        "\t•\tAccuracy = 0,7478\n",
        "\t•\tCoût métier (10×FN + 1×FP) : 32 320\n",
        "→ Coût moyen par client ≈ 0,5255\n",
        "\n",
        "⸻\n",
        "\n",
        "Reformulons la problématique :\n",
        "\n",
        "“On veut un modèle qui limite les prêts accordés à des gens qui ne remboursent pas (FN), en acceptant de refuser certains bons clients, avec un coût FN = 10 × FP.”\n",
        "\n",
        "Regardons ça avec des comparaisons simples.\n",
        "\n",
        "3.1. Si la banque n’utilise aucun modèle\n",
        "\n",
        "Deux politiques naïves :\n",
        "\t1.\tAccorder TOUS les crédits (toujours 0)\n",
        "\t•\tFN = tous les mauvais payeurs = 4 965\n",
        "\t•\tFP = 0\n",
        "\t•\tCoût = 10 × 4 965 = 49 650\n",
        "\t•\tCoût moyen ≈ 0,807 par client\n",
        "\t2.\tRefuser TOUS les crédits (toujours 1)\n",
        "\t•\tFN = 0\n",
        "\t•\tFP = tous les bons = 56 538\n",
        "\t•\tCoût = 56 538\n",
        "\t•\tCoût moyen ≈ 0,919 par client\n",
        "\n",
        "Notre modèle :\n",
        "\t•\tCoût = 32 320\n",
        "\t•\tCoût moyen ≈ 0,5255\n",
        "\n",
        "Conclusion :\n",
        "Le modèle fait nettement mieux que les deux politiques extrêmes.\n",
        "Il réduit le coût moyen par client de ~0,81 → ~0,53, donc baisse de ~35 % par rapport à “on accepte tout”.\n",
        "\n",
        "⸻\n",
        "\n",
        "Que devient le risque dans le portefeuille accepté ?\n",
        "\n",
        "Parmi les clients acceptés (prédits 0) :\n",
        "\t•\tAcceptés = TN + FN = 42 898 + 1 868 = 44 766\n",
        "\t•\tParmi eux, mauvais payeurs = FN = 1 868\n",
        "\n",
        "Taux de mauvais payeurs parmi les acceptés :\n",
        "\n",
        "{Default rate parmi acceptés} = {1868}/{44766} \\approx 4,2\\%\n",
        "\n",
        "Alors que dans la population totale :\n",
        "\n",
        "{Default rate global} = {4965}{61503} \\approx 8,1\\%\n",
        "\n",
        " Notre modèle champion a quasiment divisé par 2 le taux de mauvais payeurs dans le portefeuille de crédits accordés.\n",
        "\n",
        "Ça, pour un métier risque crédit, c’est très parlant :\n",
        "\t•\t“On garde 73 % des demandes acceptées”\n",
        "\t•\tmais\n",
        "\t•\t“On divise par 2 la proportion de mauvais payeurs dans ce portefeuille”\n",
        "\n",
        "⸻\n",
        "\n",
        "Quel est le prix à payer côté manque à gagner (FP) ?\n",
        "\n",
        "Parmi les bons clients (classe 0, 56 538 au total) :\n",
        "\t•\tTN = 42 898 → bons acceptés\n",
        "\t•\tFP = 13 640 → bons refusés (manque à gagner)\n",
        "\n",
        "Taux de bons refusés :\n",
        "\n",
        "{13\\,640}{56\\,538} \\approx 24\\%\n",
        "\n",
        "Donc :\n",
        "\t•\tEnviron 76 % des bons clients sont acceptés\n",
        "\t•\tEnviron 24 % des bons clients sont refusés  (FP)\n",
        "\n",
        "Vu que :\n",
        "\t•\tFN “coûtent 10”\n",
        "\t•\tFP “coûtent 1”\n",
        "\n",
        "Avec ce modèle et ce seuil, on :\n",
        "\t•\taccepte ~73 % des demandes de crédit,\n",
        "\t•\ttout en réduisant de moitié le taux de mauvais payeurs parmi les clients réellement financés,\n",
        "\t•\tpour un coût métier nettement inférieur aux politiques naïves (acceptation ou refus systématique).\n",
        "Ce compromis est cohérent avec l’hypothèse de départ : un défaut coûte environ 10 fois plus cher qu’un refus injustifié.\n",
        "\n",
        "\n",
        "\n",
        "\t•\tSi la banque veut encore moins de mauvais payeurs acceptés (FN), on peut :\n",
        "\t•\taugmenter le poids c_fn (>10),\n",
        "\t•\tou choisir un seuil donnant un recall1 plus élevé, même si le coût global remonte légèrement.\n",
        "\t•\tSi au contraire elle veut refuser moins de bons clients, on peut :\n",
        "\t•\tréduire c_fn/c_fp,\n",
        "\t•\tou choisir un seuil un peu moins conservateur (rappel 1 plus bas, mais moins de FP).\n",
        "\n",
        "\n",
        "j’ai calibré le modèle sur un ratio de coût FN/FP = 10.\n",
        "Si le métier juge que ce ratio est trop/peu conservateur, on peut facilement recalibrer le seuil ou les poids pour adapter le modèle à leur tolérance au risque.\n",
        "\n",
        "⸻\n",
        "\n",
        "\n",
        "“Le modèle XGBoost sélectionné, avec un seuil optimisé à 0,54, permet :\n",
        "\t•\td’atteindre une AUC de 0,76 (bonne capacité de discrimination),\n",
        "\t•\tde refuser ~62 % des clients qui auraient effectivement fait défaut,\n",
        "\t•\tde réduire le taux de défaut parmi les crédits accordés d’environ 8,1 % à 4,2 %,\n",
        "\t•\ttout en acceptant encore ~73 % des demandes de crédit.\n",
        "\n",
        "Le coût métier moyen par client (FN pondérés 10 fois plus que les FP) est significativement inférieur aux stratégies naïves (tout accepter ou tout refuser).\n",
        "On répond donc à la problématique métier en limitant les pertes en capital liées aux défauts, au prix d’un certain nombre de refus sur des clients solvables, \n",
        "ce qui est conforme au cadrage initial.\n",
        "\n",
        "\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29733c8c",
      "metadata": {},
      "source": [
        "## <a id=\"8-suivi-experimental-avec-mlflow\"></a>8. Suivi expérimental avec MLflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd426666",
      "metadata": {},
      "source": [
        "# Configuration MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328c36e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configure MLflow tracking\n",
        "mlflow.set_tracking_uri(MLFLOW_DIR.as_uri())\n",
        "mlflow.set_experiment(\"home_credit_scoring\")\n",
        "\n",
        "experiment_name = \"home_credit_scoring\"\n",
        "\n",
        "exp = mlflow.get_experiment_by_name(experiment_name)\n",
        "if exp is None:\n",
        "    experiment_id = mlflow.create_experiment(experiment_name)\n",
        "else:\n",
        "    experiment_id = exp.experiment_id\n",
        "\n",
        "print(\"Experiment ID utilisé :\", experiment_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfaa7e86",
      "metadata": {},
      "source": [
        "## Rassembler les infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5495ab83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consolidate champion model information\n",
        "champion_info = {\n",
        "    'model_name': best_model_name,\n",
        "    'best_threshold': best_threshold,\n",
        "    'best_params': best_params,\n",
        "    # Validation metrics\n",
        "    'valid_auc': optimal_results[best_model_name]['auc_valid'],\n",
        "    'valid_cost': optimal_results[best_model_name]['cost'],\n",
        "    'valid_avg_cost_per_customer': optimal_results[best_model_name]['avg_cost_per_customer'],\n",
        "    'valid_recall_1': optimal_results[best_model_name]['recall_1'],\n",
        "    'valid_precision_1': optimal_results[best_model_name]['precision_1'],\n",
        "    'valid_f1_1': optimal_results[best_model_name]['f1_1'],\n",
        "    'valid_confusion_matrix': optimal_results[best_model_name]['confusion_matrix'],\n",
        "    'valid_tp': optimal_results[best_model_name]['tp'],\n",
        "    'valid_fp': optimal_results[best_model_name]['fp'],\n",
        "    'valid_tn': optimal_results[best_model_name]['tn'],\n",
        "    'valid_fn': optimal_results[best_model_name]['fn'],\n",
        "    # Test metrics\n",
        "    'test_auc': final_test_results['auc_test'],\n",
        "    'test_cost': final_test_results['cost_test'],\n",
        "    'test_avg_cost_per_customer': final_test_results['avg_cost_per_customer_test'],\n",
        "    'test_recall_1': final_test_results['recall_1_test'],\n",
        "    'test_precision_1': final_test_results['precision_1_test'],\n",
        "    'test_f1_1': final_test_results['f1_1_test'],\n",
        "    'test_accuracy': final_test_results['accuracy_test'],\n",
        "    'test_confusion_matrix': final_test_results['confusion_matrix_test'],\n",
        "    'test_tp': final_test_results['tp'],\n",
        "    'test_fp': final_test_results['fp'],\n",
        "    'test_tn': final_test_results['tn'],\n",
        "    'test_fn': final_test_results['fn'],\n",
        "}\n",
        "\n",
        "print(\"Champion model summary:\")\n",
        "for key, value in champion_info.items():\n",
        "    if isinstance(value, (float, int)):\n",
        "        print(f\"  {key}: {value}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "champion_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b27cf3",
      "metadata": {},
      "source": [
        "## Run MLflow pour le modèle champion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c04a2aea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "import mlflow.sklearn\n",
        "\n",
        "#  Variables importantes à passer à MLflow : \n",
        "# - best_model_name\n",
        "# - best_threshold\n",
        "# - best_params\n",
        "# - champion_info\n",
        "# - final_pipeline           (pipeline champion entraîné sur X_train_full)\n",
        "# - y_test, y_proba_test\n",
        "# - y_valid, thresholds      (liste des seuils explorés)\n",
        "# - evaluate_with_cost\n",
        "# - y_proba_dict             (probas sur X_valid pour chaque modèle)\n",
        "# - scale_pos_weight         (pour XGBoost)\n",
        "\n",
        "mlflow_run_name = f\"champion_{best_model_name}\"\n",
        "print(f\"Logging MLflow run: {mlflow_run_name}\")\n",
        "\n",
        "FIGURES_DIR = Path(\"/Users/ely/Developer/home_credit_project01/home_credit_project/artifacts/figures\")\n",
        "\n",
        "\n",
        "# Pour la courbe coût vs seuil du champion sur validation\n",
        "y_proba_valid_champion = y_proba_dict[best_model_name]\n",
        "\n",
        "with mlflow.start_run(\n",
        "    run_name=mlflow_run_name,\n",
        "    experiment_id=experiment_id,):\n",
        "    # ───────────────────────────── Params ─────────────────────────────\n",
        "    param_payload = {\n",
        "        \"model_name\": best_model_name,\n",
        "        \"best_threshold\": best_threshold,\n",
        "        \"c_fn\": 10,\n",
        "        \"c_fp\": 1,\n",
        "        \"scale_pos_weight\": scale_pos_weight,\n",
        "        \"n_train_full\": len(X_train_full),\n",
        "        \"n_valid\": len(y_valid),\n",
        "        \"n_test\": len(y_test),\n",
        "        # hyperparamètres XGBoost (castés en str pour être sûrs)\n",
        "        **{str(k): str(v) for k, v in best_params.items()},\n",
        "    }\n",
        "    mlflow.log_params(param_payload)\n",
        "\n",
        "    # ───────────────────────────── Metrics ─────────────────────────────\n",
        "    # Validation\n",
        "    validation_metrics = {\n",
        "        \"valid_auc\": champion_info[\"valid_auc\"],\n",
        "        \"valid_cost\": champion_info[\"valid_cost\"],\n",
        "        \"valid_avg_cost_per_customer\": champion_info[\"valid_avg_cost_per_customer\"],\n",
        "        \"valid_recall_1\": champion_info[\"valid_recall_1\"],\n",
        "        \"valid_precision_1\": champion_info[\"valid_precision_1\"],\n",
        "        \"valid_f1_1\": champion_info[\"valid_f1_1\"],\n",
        "    }\n",
        "\n",
        "    # Test\n",
        "    test_metrics = {\n",
        "        \"test_auc\": champion_info[\"test_auc\"],\n",
        "        \"test_cost\": champion_info[\"test_cost\"],\n",
        "        \"test_avg_cost_per_customer\": champion_info[\"test_avg_cost_per_customer\"],\n",
        "        \"test_recall_1\": champion_info[\"test_recall_1\"],\n",
        "        \"test_precision_1\": champion_info[\"test_precision_1\"],\n",
        "        \"test_f1_1\": champion_info[\"test_f1_1\"],\n",
        "        \"test_accuracy\": champion_info[\"test_accuracy\"],\n",
        "    }\n",
        "\n",
        "    mlflow.log_metrics({**validation_metrics, **test_metrics})\n",
        "\n",
        "    # ───────────── Confusion matrices (JSON artefact) ─────────────\n",
        "    mlflow.log_dict(\n",
        "        {\n",
        "            \"validation_confusion_matrix\": champion_info[\n",
        "                \"valid_confusion_matrix\"\n",
        "            ].tolist(),\n",
        "            \"test_confusion_matrix\": champion_info[\"test_confusion_matrix\"].tolist(),\n",
        "            \"validation_counts\": {\n",
        "                \"tp\": champion_info[\"valid_tp\"],\n",
        "                \"fp\": champion_info[\"valid_fp\"],\n",
        "                \"tn\": champion_info[\"valid_tn\"],\n",
        "                \"fn\": champion_info[\"valid_fn\"],\n",
        "            },\n",
        "            \"test_counts\": {\n",
        "                \"tp\": champion_info[\"test_tp\"],\n",
        "                \"fp\": champion_info[\"test_fp\"],\n",
        "                \"tn\": champion_info[\"test_tn\"],\n",
        "                \"fn\": champion_info[\"test_fn\"],\n",
        "            },\n",
        "        },\n",
        "        artifact_file=\"confusion_matrices.json\",\n",
        "    )\n",
        "\n",
        "    # ───────────── ROC curve (TEST) ─────────────\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"ROC test (AUC = {champion_info['test_auc']:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve - Test set\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    roc_path = FIGURES_DIR / \"roc_curve_test.png\"\n",
        "    plt.savefig(roc_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(str(roc_path))\n",
        "\n",
        "    # ───────────── Coût métier vs seuil (VALIDATION) ─────────────\n",
        "    costs = []\n",
        "    for t in thresholds:\n",
        "        res_t = evaluate_with_cost(y_valid, y_proba_valid_champion, threshold=t, c_fn=10, c_fp=1)\n",
        "        costs.append(res_t[\"total_cost\"])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(thresholds, costs, marker=\".\")\n",
        "    plt.axvline(best_threshold, color=\"red\", linestyle=\"--\", label=f\"Best threshold = {best_threshold:.2f}\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Total cost (10*FN + 1*FP)\")\n",
        "    plt.title(\"Business cost vs threshold (validation)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    cost_curve_path = FIGURES_DIR /  \"cost_vs_threshold_valid.png\"\n",
        "    plt.savefig(cost_curve_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(str(cost_curve_path))\n",
        "\n",
        "    # ───────────── Confusion matrix (TEST) en heatmap ─────────────\n",
        "    cm_test = champion_info[\"test_confusion_matrix\"]\n",
        "    plt.figure()\n",
        "    plt.imshow(cm_test, interpolation=\"nearest\")\n",
        "    plt.title(\"Confusion Matrix - Test set\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = [0, 1]\n",
        "    plt.xticks(tick_marks, [\"Pred 0\", \"Pred 1\"])\n",
        "    plt.yticks(tick_marks, [\"True 0\", \"True 1\"])\n",
        "\n",
        "    # annotations\n",
        "    for i in range(cm_test.shape[0]):\n",
        "        for j in range(cm_test.shape[1]):\n",
        "            plt.text(\n",
        "                j,\n",
        "                i,\n",
        "                int(cm_test[i, j]),\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                color=\"white\" if cm_test[i, j] > cm_test.max() / 2 else \"black\",\n",
        "            )\n",
        "\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    cm_path = FIGURES_DIR /  \"confusion_matrix_test.png\"\n",
        "    plt.savefig(cm_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(str(cm_path))\n",
        "\n",
        "    # ───────────────────────────── SHAP Values ─────────────────────────────\n",
        "    \n",
        "    # Extraire le modèle et le preprocessor de la pipeline\n",
        "    model = final_pipeline.named_steps['classifier']\n",
        "    preprocessor_steps = final_pipeline[:-1]\n",
        "    \n",
        "    # Transformer les données test (on utilise un échantillon pour la rapidité)\n",
        "    X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)\n",
        "    X_test_transformed = preprocessor_steps.transform(X_test_sample)\n",
        "    feature_names = preprocessor_steps.get_feature_names_out()\n",
        "    \n",
        "    # Calcul SHAP\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_test_transformed)\n",
        "    \n",
        "    # Pour XGBoost binaire, shap_values est directement un array (pas une liste)\n",
        "    # Mais on vérifie au cas où\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = shap_values[1]\n",
        "    \n",
        "    # 1. Summary plot (beeswarm)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, max_display=20, show=False)\n",
        "    plt.tight_layout()\n",
        "    shap_summary_path = FIGURES_DIR /  \"shap_summary_plot.png\"\n",
        "    plt.savefig(shap_summary_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.show()  # Affiche dans le notebook\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(str(shap_summary_path))\n",
        "    \n",
        "    # 2. Bar plot (importance moyenne absolue)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, plot_type=\"bar\", max_display=20, show=False)\n",
        "    plt.tight_layout()\n",
        "    shap_bar_path = FIGURES_DIR /  \"shap_feature_importance.png\"\n",
        "    plt.savefig(shap_bar_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(str(shap_bar_path))\n",
        "    \n",
        "    print(\"SHAP plots logged to MLflow.\")\n",
        "\n",
        "\n",
        "    # ───────────── Enregistrement du modèle champion ─────────────\n",
        "    # Pipeline complet (préprocesseur + XGBoost) dans le Model Registry\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=final_pipeline,\n",
        "        artifact_path=\"model\",\n",
        "        registered_model_name=\"home_credit_xgb_champion\",\n",
        "    )\n",
        "\n",
        "print(\"MLflow logging for champion model completed.\")\n",
        "\"\"\" \n",
        "\n",
        "Je crée un run MLflow dédié au modèle champion et j’y loggue :\n",
        "\t•\tles hyperparamètres issus du RandomizedSearchCV,\n",
        "\t•\tle seuil métier optimisé,\n",
        "\t•\tles métriques sur le jeu de validation,\n",
        "\t•\tet les métriques finales sur le jeu de test.\n",
        "également les features importances via SHAP (summary plot et bar plot).\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af3debe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "cwd = Path.cwd()\n",
        "PROJECT_ROOT = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
        "\n",
        "MODELS_DIR = PROJECT_ROOT / \"artifacts\" / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PIPELINE_PATH = MODELS_DIR / \"champion_pipeline.joblib\"\n",
        "METADATA_PATH = MODELS_DIR / \"champion_metadata.json\"\n",
        "\n",
        "assert \"final_pipeline\" in globals(), \"final_pipeline introuvable.\"\n",
        "assert \"best_threshold\" in globals(), \"best_threshold introuvable.\"\n",
        "\n",
        "# --------- 3) Sauvegarder le pipeline sklearn (préprocess + modèle) ---------\n",
        "joblib.dump(final_pipeline, PIPELINE_PATH)\n",
        "\n",
        "# --------- 4) Sauvegarder les métadonnées (seuil + infos utiles) ---------\n",
        "def _to_jsonable(x):\n",
        "    # convertit types numpy/pandas en types JSON-friendly\n",
        "    try:\n",
        "        import numpy as np\n",
        "        if isinstance(x, (np.integer,)):\n",
        "            return int(x)\n",
        "        if isinstance(x, (np.floating,)):\n",
        "            return float(x)\n",
        "        if isinstance(x, (np.ndarray,)):\n",
        "            return x.tolist()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "metadata = {\n",
        "    \"exported_at\": dt.datetime.now().isoformat(),\n",
        "    \"best_threshold\": float(best_threshold),\n",
        "    \"c_fn\": 10.0,\n",
        "    \"c_fp\": 1.0,\n",
        "    \"model_name\": globals().get(\"best_model_name\", \"unknown\"),\n",
        "    \"best_params\": {str(k): _to_jsonable(v) for k, v in globals().get(\"best_params\", {}).items()},\n",
        "}\n",
        "\n",
        "with METADATA_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# --------- 5) Afficher un résumé ---------\n",
        "print(\"✅ Champion exporté !\")\n",
        "print(\"Pipeline :\", PIPELINE_PATH)\n",
        "print(\"Metadata :\", METADATA_PATH)\n",
        "print(f\"Pipeline size: {PIPELINE_PATH.stat().st_size/1024/1024:.2f} MB\")\n",
        "print(\"Threshold :\", metadata[\"best_threshold\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
